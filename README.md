# Seminar-1-The-Transformer-Architecture-Facilitating-Effective-Pretraining-and-Task-Adaptation

Seminar 1 provides a comprehensive overview of five papers relevant to the Transformer architecture, conducted under the mentorship of Prof. Dr. Sc. Jan Šnajder and Mag. Ing. David Dukć, as part of the curriculum at FER.

## Papers:
- [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)
- [Improving language understanding by generative pre-training (Radford et al., 2018)](https://www.mikecaptain.com/resources/pdf/GPT-1.pdf)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
- [Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks (Gururangan et al., 2020)](https://arxiv.org/abs/2004.10964)
- [TaCL: Improving BERT Pre-training with Token-aware Contrastive Learning (Su et al., 2021)](https://arxiv.org/abs/2111.04198)
