# Seminar-1-The-Transformer-Architecture-Facilitating-Effective-Pretraining-and-Task-Adaptation
Seminar 1 is a comprehensive overview of five papers relevant to the Transformer architecture, conducted under the mentorship of prof. dr. sc. Jan Šnajder and mag. ing. David Dukć, as a part of the curriculum at FER.
Papers:
- **1**: [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)
- **2**: [Improving language understanding by generative pre-training (Radford et al., 2018)](https://www.mikecaptain.com/resources/pdf/GPT-1.pdf)
- **3**: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
- **4**: [Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks (Gururangan et al., 2020)](https://arxiv.org/abs/2004.10964)
- **5**: [TaCL: Improving BERT Pre-training with Token-aware Contrastive Learning (Su et al., 2021)](https://arxiv.org/abs/2111.04198)
