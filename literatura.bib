@article{vaswani2017attention,
    title={Attention is all you need},
    author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
    journal={Advances in neural information processing systems},
    volume={30},
    year={2017}
}

@article{chung2014empirical,
    title={Empirical evaluation of gated recurrent neural networks on sequence modeling},
    author={Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
    journal={arXiv preprint arXiv:1412.3555},
    year={2014}
}

@article{bahdanau2014neural,
    title={Neural machine translation by jointly learning to align and translate},
    author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
    journal={arXiv preprint arXiv:1409.0473},
    year={2014}
}

@article{marcus1993building,
    title={Building a large annotated corpus of English: The Penn Treebank},
    author={Marcus, Mitchell and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
    year={1993}
}

@inproceedings{papineni2002bleu,
    title={Bleu: a method for automatic evaluation of machine translation},
    author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
    booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
    pages={311--318},
    year={2002}
}

@article{kuncoro2016recurrent,
     title={What do recurrent neural network grammars learn about syntax?},
    author={Kuncoro, Adhiguna and Ballesteros, Miguel and Kong, Lingpeng and Dyer, Chris and Neubig, Graham and Smith, Noah A},
    journal={arXiv preprint arXiv:1611.05774},
    year={2016}
}

@article{radford2018improving,
    title={Improving language understanding by generative pre-training},
    author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
    year={2018},
    publisher={OpenAI}
}

@phdthesis{liang2005semi,
    title={Semi-supervised learning for natural language},
    author={Liang, Percy},
    year={2005},
    school={Massachusetts Institute of Technology}
}

@article{peters2017semi,
    title={Semi-supervised sequence tagging with bidirectional language models},
    author={Peters, Matthew E and Ammar, Waleed and Bhagavatula, Chandra and Power, Russell},
    journal={arXiv preprint arXiv:1705.00108},
    year={2017}
}

@article{rei2017semi,
    title={Semi-supervised multitask learning for sequence labeling},
    author={Rei, Marek},
    journal={arXiv preprint arXiv:1704.07156},
    year={2017}
}

@inproceedings{zhu2015aligning,
    title={Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
    author={Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
    booktitle={Proceedings of the IEEE international conference on computer vision},
    pages={19--27},
    year={2015}
}

@article{bowman2015large,
    title={A large annotated corpus for learning natural language inference},
    author={Bowman, Samuel R and Angeli, Gabor and Potts, Christopher and Manning, Christopher D},
    journal={arXiv preprint arXiv:1508.05326},
    year={2015}
}

@article{bentivogli2009fifth,
    title={The Fifth PASCAL Recognizing Textual Entailment Challenge.},
    author={Bentivogli, Luisa and Clark, Peter and Dagan, Ido and Giampiccolo, Danilo},
    journal={TAC},
    volume={7},
    number={8},
    pages={1},
    year={2009}
}

@article{lai2017race,
    title={Race: Large-scale reading comprehension dataset from examinations},
    author={Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
    journal={arXiv preprint arXiv:1704.04683},
    year={2017}
}

@inproceedings{mostafazadeh2017lsdsem,
    title={Lsdsem 2017 shared task: The story cloze test},
    author={Mostafazadeh, Nasrin and Roth, Michael and Louis, Annie and Chambers, Nathanael and Allen, James},
    booktitle={Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics},
    pages={46--51},
    year={2017}
}

@inproceedings{dolan2005automatically,
    title={Automatically constructing a corpus of sentential paraphrases},
    author={Dolan, Bill and Brockett, Chris},
    booktitle={Third International Workshop on Paraphrasing (IWP2005)},
    year={2005}
}

@misc{quoraquestion,
    author = {Chen, Z. and Zhang, H. and Zhang, X. and Zhao, L.},
    title = {Quora question pairs},
    url ={https://data.quora.com/First-QuoraDataset-Release-Question-Pairs},
    year = {2018}
}

@inproceedings{socher2013recursive,
    title={Recursive deep models for semantic compositionality over a sentiment treebank},
    author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
    booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
    pages={1631--1642},
    year={2013}
}

@misc{warstadt2018corpus,
    title={Corpus of linguistic acceptability},
    author={Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R},
    year={2018}
}

@article{devlin2018bert,
    title={Bert: Pre-training of deep bidirectional transformers for language understanding},
    author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
    journal={arXiv preprint arXiv:1810.04805},
    year={2018}
}

@inproceedings{peters-etal-2018-deep,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew E.  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1202",
    doi = "10.18653/v1/N18-1202",
    pages = "2227--2237",
    abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
}

@article{wu2016google,
    title={Google's neural machine translation system: Bridging the gap between human and machine translation},
    author={Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
    journal={arXiv preprint arXiv:1609.08144},
    year={2016}
}

@article{rajpurkar2016squad,
    title={Squad: 100,000+ questions for machine comprehension of text},
    author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
    journal={arXiv preprint arXiv:1606.05250},
    year={2016}
}

@article{zellers2018swag,
    title={Swag: A large-scale adversarial dataset for grounded commonsense inference},
    author={Zellers, Rowan and Bisk, Yonatan and Schwartz, Roy and Choi, Yejin},
    journal={arXiv preprint arXiv:1808.05326},
    year={2018}
}

@article{gururangan2020don,
    title={Don't stop pretraining: Adapt language models to domains and tasks},
    author={Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A},
    journal={arXiv preprint arXiv:2004.10964},
    year={2020}
}

@article{lee2020biobert,
    title={BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
    author={Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
    journal={Bioinformatics},
    volume={36},
    number={4},
    pages={1234--1240},
    year={2020},
    publisher={Oxford University Press}
}

@article{liu2019roberta,
    title={Roberta: A robustly optimized bert pretraining approach},
    author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
    journal={arXiv preprint arXiv:1907.11692},
    year={2019}
}

@article{gururangan2019variational,
    title={Variational pretraining for semi-supervised text classification},
    author={Gururangan, Suchin and Dang, Tam and Card, Dallas and Smith, Noah A},
    journal={arXiv preprint arXiv:1906.02242},
    year={2019}
}

@article{su2021tacl,
    title={Tacl: Improving BERT pre-training with token-aware contrastive learning},
    author={Su, Yixuan and Liu, Fangyu and Meng, Zaiqiao and Lan, Tian and Shu, Lei and Shareghi, Ehsan and Collier, Nigel},
    journal={arXiv preprint arXiv:2111.04198},
    year={2021}
}

@article{giorgi2020declutr,
    title={Declutr: Deep contrastive learning for unsupervised textual representations},
    author={Giorgi, John and Nitski, Osvald and Wang, Bo and Bader, Gary},
    journal={arXiv preprint arXiv:2006.03659},
    year={2020}
}

@article{weischedel2011ontonotes,
    title={Ontonotes release 4.0},
    author={Weischedel, Ralph and Pradhan, Sameer and Ramshaw, Lance and Palmer, Martha and Xue, Nianwen and Marcus, Mitchell and Taylor, Ann and Greenberg, Craig and Hovy, Eduard and Belvin, Robert and others},
    journal={LDC2011T03, Philadelphia, Penn.: Linguistic Data Consortium},
    year={2011}
}

@inproceedings{levow2006third,
    title={The third international Chinese language processing bakeoff: Word segmentation and named entity recognition},
    author={Levow, Gina-Anne},
    booktitle={Proceedings of the Fifth SIGHAN workshop on Chinese language processing},
    pages={108--117},
    year={2006}
}

@inproceedings{emerson2005second,
    title={The second international chinese word segmentation bakeoff},
    author={Emerson, Thomas},
    booktitle={Proceedings of the fourth SIGHAN workshop on Chinese language Processing},
    year={2005}
}

@article{liu2021fast,
    title={Fast, effective, and self-supervised: Transforming masked language models into universal lexical and sentence encoders},
    author={Liu, Fangyu and Vuli{\'c}, Ivan and Korhonen, Anna and Collier, Nigel},
    journal={arXiv preprint arXiv:2104.08027},
    year={2021}
}

@article{howard2018universal,
  title={Universal language model fine-tuning for text classification},
  author={Howard, Jeremy and Ruder, Sebastian},
  journal={arXiv preprint arXiv:1801.06146},
  year={2018}
}

@article{phang2018sentence,
  title={Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks},
  author={Phang, Jason and F{\'e}vry, Thibault and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1811.01088},
  year={2018}
}